{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP Tutorial - Text Representation: TF-IDF\n",
    "\n",
    "What is TF-IDF?\n",
    "\n",
    "TF stands for Term Frequency and denotes the ratio of number of times a particular word appeared in a Document to total number of words in the document.\n",
    "\n",
    "\n",
    "   Term Frequency(TF) = [number of times word appeared / total no of words in a document]\n",
    "\n",
    "\n",
    "Term Frequency values ranges between 0 and 1. If a word occurs more number of times, then it's value will be close to 1.\n",
    "\n",
    "\n",
    "IDF stands for Inverse Document Frequency and denotes the log of ratio of total number of documents/datapoints in the whole dataset to the number of documents that contains the particular word.\n",
    "\n",
    "\n",
    "   Inverse Document Frequency(IDF) = [log(Total number of documents / number of documents that contains the word)]\n",
    "\n",
    "\n",
    "In IDF, if a word occured in more number of documents and is common across all documents, then it's value will be less and ratio will approaches to 0.\n",
    "\n",
    "\n",
    "Finally:\n",
    "\n",
    "\n",
    "   TF-IDF = Term Frequency(TF) * Inverse Document Frequency(IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thor eating pizza, Loki is eating pizza, Ironman ate pizza already',\n",
       " 'Apple is announcing new iphone tomorrow',\n",
       " 'Tesla is announcing new model-3 tomorrow',\n",
       " 'Google is announcing new pixel-6 tomorrow',\n",
       " 'Microsoft is announcing new surface tomorrow',\n",
       " 'Amazon is announcing new eco-dot tomorrow',\n",
       " 'I am eating biryani and you are eating grapes']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"Thor eating pizza, Loki is eating pizza, Ironman ate pizza already\",\n",
    "    \"Apple is announcing new iphone tomorrow\",\n",
    "    \"Tesla is announcing new model-3 tomorrow\",\n",
    "    \"Google is announcing new pixel-6 tomorrow\",\n",
    "    \"Microsoft is announcing new surface tomorrow\",\n",
    "    \"Amazon is announcing new eco-dot tomorrow\",\n",
    "    \"I am eating biryani and you are eating grapes\"\n",
    "]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's create the vectorizer and fit the corpus and transform them accordingly\n",
    "v = TfidfVectorizer()\n",
    "v.fit(corpus)\n",
    "transform_output = v.transform(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'thor': 25, 'eating': 10, 'pizza': 22, 'loki': 17, 'is': 16, 'ironman': 15, 'ate': 7, 'already': 0, 'apple': 5, 'announcing': 4, 'new': 20, 'iphone': 14, 'tomorrow': 26, 'tesla': 24, 'model': 19, 'google': 12, 'pixel': 21, 'microsoft': 18, 'surface': 23, 'amazon': 2, 'eco': 11, 'dot': 9, 'am': 1, 'biryani': 8, 'and': 3, 'you': 27, 'are': 6, 'grapes': 13}\n"
     ]
    }
   ],
   "source": [
    "#let's print the vocabulary\n",
    "\n",
    "print(v.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "v_feature = v.get_feature_names_out()\n",
    "v_len = len(v_feature)\n",
    "print(v_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already : 2.386294361119891\n",
      "am : 2.386294361119891\n",
      "amazon : 2.386294361119891\n",
      "and : 2.386294361119891\n",
      "announcing : 1.2876820724517808\n",
      "apple : 2.386294361119891\n",
      "are : 2.386294361119891\n",
      "ate : 2.386294361119891\n",
      "biryani : 2.386294361119891\n",
      "dot : 2.386294361119891\n",
      "eating : 1.9808292530117262\n",
      "eco : 2.386294361119891\n",
      "google : 2.386294361119891\n",
      "grapes : 2.386294361119891\n",
      "iphone : 2.386294361119891\n",
      "ironman : 2.386294361119891\n",
      "is : 1.1335313926245225\n",
      "loki : 2.386294361119891\n",
      "microsoft : 2.386294361119891\n",
      "model : 2.386294361119891\n",
      "new : 1.2876820724517808\n",
      "pixel : 2.386294361119891\n",
      "pizza : 2.386294361119891\n",
      "surface : 2.386294361119891\n",
      "tesla : 2.386294361119891\n",
      "thor : 2.386294361119891\n",
      "tomorrow : 1.2876820724517808\n",
      "you : 2.386294361119891\n"
     ]
    }
   ],
   "source": [
    "# let's print the idf of each word\n",
    "\n",
    "all_feature_names = v.get_feature_names_out()\n",
    "\n",
    "for word in all_feature_names:\n",
    "    #let's get the index in  the vacabulary\n",
    "    indx = v.vocabulary_.get(word)\n",
    "    #get the  score\n",
    "    idf_score = v.idf_[indx]\n",
    "\n",
    "    print(f\"{word} : {idf_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thor eating pizza, Loki is eating pizza, Ironman ate pizza already',\n",
       " 'Apple is announcing new iphone tomorrow']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.24266547, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.24266547, 0.        , 0.        ,\n",
       "        0.40286636, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.24266547, 0.11527033, 0.24266547, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.72799642, 0.        , 0.        ,\n",
       "        0.24266547, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.30652086,\n",
       "        0.5680354 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.5680354 ,\n",
       "        0.        , 0.26982671, 0.        , 0.        , 0.        ,\n",
       "        0.30652086, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.30652086, 0.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_output.toarray()[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem Statement: Given a description about a product sold on e-commerce website, classify it in one of the 4 categories\n",
    "\n",
    "Dataset Credits: https://www.kaggle.com/datasets/saurabhshahane/ecommerce-text-classification\n",
    "\n",
    "\n",
    "This data consists of two columns.\n",
    "\n",
    "Text\tLabel\n",
    "\n",
    "Indira Designer Women's Art Mysore Silk Saree With Blouse Piece (Star-Red) This Saree Is Of Art Mysore Silk & Comes With Blouse Piece.\tClothing & Accessories\n",
    "IO Crest SY-PCI40010 PCI RAID Host Controller Card Brings new life to any old desktop PC. Connects up to 4 SATA II high speed SATA hard disk drives. Supports Windows 8 and Server 2012\tElectronics\n",
    "Operating Systems in Depth About the Author Professor Doeppner is an associate professor of computer science at Brown University. His research interests include mobile computing in education, mobile and ubiquitous computing, operating systems and distribution systems, parallel computing, and security.\tBooks\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*Text*: Description of an item sold on e-commerce website\n",
    "\n",
    "\n",
    "*Label*: Category of that item. Total 4 categories: \"Electronics\", \"Household\", \"Books\" and \"Clothing & Accessories\", which almost cover 80% of any E-commerce website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read the data into a pandas dataframe\n",
    "\n",
    "df = pd.read_csv(\"ecommerceDataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50425, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "# df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Paper Plane Design Framed Wall Hanging Motivat...</td>\n",
       "      <td>Household</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SAF 'Floral' Framed Painting (Wood, 30 inch x ...</td>\n",
       "      <td>Household</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SAF 'UV Textured Modern Art Print Framed' Pain...</td>\n",
       "      <td>Household</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text      label\n",
       "0  Paper Plane Design Framed Wall Hanging Motivat...  Household\n",
       "1  SAF 'Floral' Framed Painting (Wood, 30 inch x ...  Household\n",
       "2  SAF 'UV Textured Modern Art Print Framed' Pain...  Household"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "Household                 19313\n",
       "Books                     11820\n",
       "Electronics               10621\n",
       "Clothing & Accessories     8671\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text\n",
       "Think & Grow Rich About the Author NAPOLEON HILL, born in Pound, Southwest Virginia in 1883, was a very successful American author in the area of the new thought movement—one of the earliest producers of the modern genre of personal-success literature. He is widely considered to be one of the great writers on success. The turning point in Hill’s life occurred in the year 1908 when he interviewed the industrialist Andrew Carnegie—one of the most powerful men in the world at that time, as part of an assignment—an interview which ultimately led to the publication of Think and Grow Rich, one of his best-selling books of all time. the book examines the power of personal beliefs and the role they play in personal success. Hill, who had even served as the advisor to President Franklin D. Roosevelt from 1933-36, passed away at the age of 87.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      30\n",
       "The Power of Your Subconscious Mind: Unlock Your Master Key to Success About the Author Joseph Murphy was a Divine Science minister and author. Murphy was born in Ireland, the son of a private boy's school headmaster and raised a Roman Catholic. He studied for the priesthood and joined the Jesuits. In his twenties an experience with healing prayer led him to leave the Jesuits and move to the United States. Murphy has given lectures and written books for audiences all over the world. In his lectures he points out how real people can radically improve their lives by applying specific aspects of his concepts.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              29\n",
       "HP 680 Original Ink Advantage Cartridge (Black) Colour:Black   Make the most of every memory when you use Original HP Ink cartridges for your photos as every print is developed with quality in mind. Relive great experiences from fade-resistent prints that last a lifetime when using Original HP Ink. Capture more moments with greater page yield and cost efficiencies when you print with Original HP Ink XL cartridges. Keep cartridges out of landfill by using HP's free recycling service with Cartridges 4 Planet Ark collection bins in most Retail stores.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         26\n",
       "Diverse Men's Formal Shirt Diverse is a western wear value brand for men. Our range consists of basic and updated basic apparel across both formal and casual wear. We offer the right blend of quality, style and value aimed to delight our customers.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           23\n",
       "Cubetek 2 in 1 Bluetooth Transmitter & Receiver Adapter V4.1, with Digital Optical /TOSLINK / SPDIF, 3.5mm Aux Connect, Model: CB-BTI-029 CUBETEK 2 in 1 Bluetooth Transmitter and Receiver with Bluetoth V4.1 Is a portable handy device, allows you to wirelessly stream your favourite movies, music, games, or other media from non-Bluetooth sources straight to Bluetooth headphones, or speakers. It also allows traditional audio devices like home and car stereos to receive music from your mobile or tablets.Product Feature: 1. 2-In-1 Bluetooth Adapter, One-button switch to shift between transmitter(TX) and receiver(RX) mode easily.2. This transmitter allows pairing up to 2 devices( bluetooth headset/speaker), which means you can watch TV or listen to music with others.3. Compact, lightweight and durable. Take it with you and enjoy music anytime/anywhere for up to 14 hours.Fblue Can work while charging. Micro charging port.4. Easy to Connect, Auto pairing makes your life wire-free, hassle-free 5. Multi-point and dual-streaming features: Using dual-streaming, you can send audio signals to two audio output devices simultaneously. When in receiver mode, the adapter can receive signals from two wireless audio sources with the multi-point feature. Thus, you can swiftly and conveniently swap between music libraries on both devices.Specifications: version: CSR BC8670 Bluetooth 4.1, Class Transmission range: 10 meters (up to 33 ft)Working time:TX Mode: Up to 14 hoursRX Mode: Up to 14 hoursBattery Capacity: 280 mAhCharging time: 2 hours Standby time : 250 hoursSize: 49*13mmWeight: 21gms Bluetooth Profile: A2DP, APT-X LL(TX), APT-X(TX), SBC(TX), SBC(RX) Package Includes:1 x Audio Adapter1 x Digital Optical Toslink Cable1 x 3.5Mm Audio Cable1 x Micro Usb Charging Cable1 x 3.5Mm Female to 2 RCA Male Cable1 x user Manual.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            22\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ..\n",
       "Naming Jack the Ripper About the Author Russell Edwards has a Master's Degree in Business Administration from Middlesex University and a Postgraduate Diploma in Business Administration from Westminster University. He is currently managing director of Alexander Grace (Land Assets) Ltd in the UK, a company which he founded in 2009. It enables underprivileged youths to train in modern agriculture techniques and to develop initiative and creativity. His personal interests include cinema, theatre, fitness, architecture and interior design and London history, which is how he came to research Victorian London's East End and the crimes of Jack the Ripper. He lives with his wife Sally and their two children in Hadley Wood, North London. Dr. Jari Louhelainen is a Senior Lecturer in Molecular Biology at Liverpool John Moores University in the UK, as well as Associate Professor of Biochemistry at the University of Helsinki. He has two major lines of research at Liverpool: mammalian/medical genetics, working with their sports science department of the university; and forensics, working with the forensics department. In his resume on the Liverpool John Moores University website, his expertise in the forensic area includes 'determination of age of forensic samples', 'new methods for forensic imaging applications of Next Generation sequencing for forensics' and 'human identification using novel genetic methods'.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         1\n",
       "Man-Eaters of Tsavo Review \"I think that the incident of the Uganda man-eating lions . . . is the most remarkable account of which we have any record.\" \\t\\t\\t\\t    \\t \\t\\t\\t\\t\\t From the Back Cover Originally published in 1907, this book records the harrowing true story of what really happened at Tsavo in 1898. If you are interested in the Tsavo Maneaters, this book is a must read! One of the first works on the subject of man-eating lions by someone with first-hand experience, it details a series of attacks on the Indian workers building the Mombasa to Uganda railway in 18981899. It was Patterson's task to rid the project of some man-eaters who had decimated the missing, but the discovery of the bodies soon dispelled that notion. Over the next few months, the lions had their pick of the workers along the railway line, and various traps and vigils all failed to work, until one of the lions was shot by Patterson, who had waited all night at the top of a 12 feet-high structure of sticks for the opportunity. The second male was shot soon after, but between them these two lions had accounted for over 100 workers and natives, \"of whom no official record was kept.\"After killing the infamous lions, Colonel Patterson had their skins made into rugs and they resided in his home in England after his return from East Africa. In 1907, he wrote \"The Man Eaters of Tsavo\", which was a hit, and is considered today to be a literary classic. It is still to this day the most extraordinary account of man-eating animals ever recorded!|Originally published in 1907, this book records the harrowing true story of what really happened at Tsavo in 1898. If you are interested in the Tsavo Maneaters, this book is a must read! One of the first works on the subject of man-eating lions by someone with first-hand experience, it details a series of attacks on the Indian workers building the Mombasa to Uganda railway in 18981899. It was Patterson's task to rid the project of some man-eaters who had decimated the missing, but the discovery of the bodies soon dispelled that notion. Over the next few months, the lions had their pick of the workers along the railway line, and various traps and vigils all failed to work, until one of the lions was shot by Patterson, who had waited all night at the top of a 12 feet-high structure of sticks for the opportunity. The second male was shot soon after, but between them these two lions had accounted for over 100 workers and natives, \"of whom no official record was kept.\"After killing the infamous lions, Colonel Patterson had their skins made into rugs and they resided in his home in England after his return from East Africa. In 1907, he wrote \"The Man Eaters of Tsavo\", which was a hit, and is considered today to be a literary classic. It is still to this day the most extraordinary account of man-eating animals ever recorded!     1\n",
       "Green Hills of Africa: The Hemingway Library Edition Review \"[An] account of a hunting safari on the Serengeti Plains, a chronicle of adventure and a literary challenge Hemingway set up for himself. Anticipating by decades Truman Capote's 'nonfiction novel,' the classic 'In Cold Blood,' Hemingway wanted to prove that 'an absolutely true book' can 'compete with a work of the imagination.'\" (Hillel Italie, Associated Press)“[Hemingway’s wife, Pauline’s] engaging, laconic observations offer yet another lens through which to witness Hemingway at large in the world, while also helping the reader gauge how much, or how little, Hemingway reshaped the reality of his experiences in order to express, to his own satisfaction, his fondness for the hunt, his affinity for the natural world, and his abiding love of ‘the dark continent’ itself....With its journal entries, an insightful foreword, and a moving introduction by Hemingway’s sons, and some charming ‘letters from Africa’ that Hemingway published in Esquire…the reissue of this book is an opportunity, a reminder, to dive in again to a title we probably haven’t thought about for years….Encountering the book again after all these years, it’s hard not to marvel, page after page, at Hemingway’s singular gift for pure, descriptive prose.” (The Daily Beast)“The true joy lies in reading in Hemingway’s prose again: precise, lyrical, unwinding in long sentences, suggesting more than it reveals, sumptuous in its descriptions of the valleys, ravines, salt-licks, hills and forests of his beloved Africa. What I really want to do is quote great swaths of his style at its most beautiful, hypnotic and expert.” (Sam Coale, Providence Journal) \\t\\t\\t\\t    \\t \\t\\t\\t\\t\\t About the Author Ernest Hemingway did more to influence the style of English prose than any other writer of his time. Publication of The Sun Also Rises and A Farewell to Arms immediately established him as one of the greatest literary lights of the 20th century. His classic novella The Old Man and the Sea won the Pulitzer Prize in 1953. Hemingway was awarded the Nobel Prize for Literature in 1954. He died in 1961. \\t\\t\\t\\t    \\t \\t\\t\\t\\t\\t              See all Product description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     1\n",
       "Naming Jack the Ripper About the Author Russell Edwards is a businessman and property developer who has long been fascinated by the East End of London and by the crimes of Jack the Ripper. He lives in Hertfordshire.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             1\n",
       "Green Hills of Africa                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               1\n",
       "Name: count, Length: 27802, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Text'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle Class Imbalance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples = 8500\n",
    "\n",
    "df_Household = df[df.label==\"Household\"].sample(min_samples, random_state=102)\n",
    "df_Books = df[df.label==\"Books\"].sample(min_samples, random_state=102)\n",
    "df_Electronics = df[df.label==\"Electronics\"].sample(min_samples, random_state=102)\n",
    "df_Clothing_and_Accessories = df[df.label==\"Clothing & Accessories\"].sample(min_samples, random_state=102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "Household                 8500\n",
       "Books                     8500\n",
       "Electronics               8500\n",
       "Clothing & Accessories    8500\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_balanced = pd.concat([df_Household,df_Books,df_Electronics,df_Clothing_and_Accessories],axis=0)\n",
    "df_balanced.label.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>NOVICZ Nylon Hammock Swing Hanging Rope Bed fo...</td>\n",
       "      <td>Household</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2502</th>\n",
       "      <td>Casa Decor Ceramic Decorative Filigree Wall Ho...</td>\n",
       "      <td>Household</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1529</th>\n",
       "      <td>Mollismoons Without Beans Luxury Fur and Leath...</td>\n",
       "      <td>Household</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19163</th>\n",
       "      <td>Shraddha Collections 2 Compartments Steel Cash...</td>\n",
       "      <td>Household</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9231</th>\n",
       "      <td>Skittles Gems Fruits Pouch, 174g Fruit flavore...</td>\n",
       "      <td>Household</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text      label  label_num\n",
       "828    NOVICZ Nylon Hammock Swing Hanging Rope Bed fo...  Household          0\n",
       "2502   Casa Decor Ceramic Decorative Filigree Wall Ho...  Household          0\n",
       "1529   Mollismoons Without Beans Luxury Fur and Leath...  Household          0\n",
       "19163  Shraddha Collections 2 Compartments Steel Cash...  Household          0\n",
       "9231   Skittles Gems Fruits Pouch, 174g Fruit flavore...  Household          0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add the new column which gives a unique number to each of these labels \n",
    "\n",
    "df_balanced['label_num'] = df_balanced['label'].map({\n",
    "    'Household' : 0, \n",
    "    'Books': 1, \n",
    "    'Electronics': 2, \n",
    "    'Clothing & Accessories': 3\n",
    "})\n",
    "\n",
    "#checking the results \n",
    "\n",
    "df_balanced.shape\n",
    "df_balanced.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text         1\n",
      "label        0\n",
      "label_num    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "any_null_value = df_balanced.isna().sum()\n",
    "print(any_null_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text         0\n",
      "label        0\n",
      "label_num    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# clean null value\n",
    "data_cleaned_all_null = df_balanced.dropna(how='all')\n",
    "df_balanced.dropna(inplace=True)\n",
    "\n",
    "any_null_value = df_balanced.isna().sum()\n",
    "print(any_null_value)\n",
    "# clean null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_balanced.Text,\n",
    "    df_balanced.label_num, \n",
    "    test_size=0.2, # 20% samples will go to test dataset\n",
    "    random_state=2022,\n",
    "    stratify=df_balanced.label_num\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train:  (27199,)\n",
      "Shape of X_test:  (6800,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Shape of X_train: \", X_train.shape)\n",
    "print(\"Shape of X_test: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_num\n",
       "1    6800\n",
       "0    6800\n",
       "2    6800\n",
       "3    6799\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_num\n",
       "0    1700\n",
       "1    1700\n",
       "2    1700\n",
       "3    1700\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt 1 :\n",
    "\n",
    "# using sklearn pipeline module create a classification pipeline to classify the Ecommerce Data.\n",
    "# Note:\n",
    "\n",
    "# use TF-IDF for pre-processing the text.\n",
    "\n",
    "# use KNN as the classifier\n",
    "\n",
    "# print the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.94      1700\n",
      "           1       0.97      0.95      0.96      1700\n",
      "           2       0.96      0.96      0.96      1700\n",
      "           3       0.98      0.98      0.98      1700\n",
      "\n",
      "    accuracy                           0.96      6800\n",
      "   macro avg       0.96      0.96      0.96      6800\n",
      "weighted avg       0.96      0.96      0.96      6800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#1. create a pipeline object\n",
    "clf = Pipeline([\n",
    "     ('vectorizer_tfidf',TfidfVectorizer()),    \n",
    "     ('KNN', KNeighborsClassifier())         \n",
    "])\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "#4. print the classfication report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.93      1700\n",
      "           1       0.97      0.92      0.95      1700\n",
      "           2       0.95      0.95      0.95      1700\n",
      "           3       0.97      0.98      0.98      1700\n",
      "\n",
      "    accuracy                           0.95      6800\n",
      "   macro avg       0.95      0.95      0.95      6800\n",
      "weighted avg       0.95      0.95      0.95      6800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "#1. create a pipeline object\n",
    "clf = Pipeline([\n",
    "     ('vectorizer_tfidf',TfidfVectorizer()),    \n",
    "     ('Multi NB', MultinomialNB())         \n",
    "])\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "#4. print the classfication report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.96      0.94      1700\n",
      "           1       0.97      0.96      0.97      1700\n",
      "           2       0.98      0.94      0.96      1700\n",
      "           3       0.98      0.99      0.98      1700\n",
      "\n",
      "    accuracy                           0.96      6800\n",
      "   macro avg       0.96      0.96      0.96      6800\n",
      "weighted avg       0.96      0.96      0.96      6800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#1. create a pipeline object\n",
    "clf = Pipeline([\n",
    "     ('vectorizer_tfidf',TfidfVectorizer()),        #using the ngram_range parameter \n",
    "     ('Random Forest', RandomForestClassifier())         \n",
    "])\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "#4. print the classfication report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use text pre-processing to remove stop words, punctuations and apply lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## utility function for pre-preprocessing the text \n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess(text):\n",
    "    doc = nlp(text)\n",
    "    filtered_tokens = []\n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct:\n",
    "            continue\n",
    "        filtered_tokens.append(token.lemma_)\n",
    "    return \" \".join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E1041] Expected a string, Doc, or bytes as input, but got: <class 'float'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mpreprocessed_text\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mText\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(preprocess)\n",
      "File \u001b[0;32m~/anaconda3/envs/bd/lib/python3.11/site-packages/pandas/core/series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4520\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4521\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4525\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4526\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4527\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4528\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4529\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4628\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4629\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4630\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m~/anaconda3/envs/bd/lib/python3.11/site-packages/pandas/core/apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m   1024\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/anaconda3/envs/bd/lib/python3.11/site-packages/pandas/core/apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1075\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m-> 1076\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1077\u001b[0m             values,\n\u001b[1;32m   1078\u001b[0m             f,\n\u001b[1;32m   1079\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1080\u001b[0m         )\n\u001b[1;32m   1082\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1083\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/anaconda3/envs/bd/lib/python3.11/site-packages/pandas/_libs/lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[27], line 8\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess\u001b[39m(text):\n\u001b[0;32m----> 8\u001b[0m     doc \u001b[39m=\u001b[39m nlp(text)\n\u001b[1;32m      9\u001b[0m     filtered_tokens \u001b[39m=\u001b[39m []\n\u001b[1;32m     10\u001b[0m     \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m doc:\n",
      "File \u001b[0;32m~/anaconda3/envs/bd/lib/python3.11/site-packages/spacy/language.py:1030\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[1;32m   1010\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1011\u001b[0m     text: Union[\u001b[39mstr\u001b[39m, Doc],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1014\u001b[0m     component_cfg: Optional[Dict[\u001b[39mstr\u001b[39m, Dict[\u001b[39mstr\u001b[39m, Any]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1015\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Doc:\n\u001b[1;32m   1016\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[39m    and can contain arbitrary whitespace. Alignment into the original string\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[39m    is preserved.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[39m    DOCS: https://spacy.io/api/language#call\u001b[39;00m\n\u001b[1;32m   1029\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1030\u001b[0m     doc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ensure_doc(text)\n\u001b[1;32m   1031\u001b[0m     \u001b[39mif\u001b[39;00m component_cfg \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1032\u001b[0m         component_cfg \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/anaconda3/envs/bd/lib/python3.11/site-packages/spacy/language.py:1124\u001b[0m, in \u001b[0;36mLanguage._ensure_doc\u001b[0;34m(self, doc_like)\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(doc_like, \u001b[39mbytes\u001b[39m):\n\u001b[1;32m   1123\u001b[0m     \u001b[39mreturn\u001b[39;00m Doc(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab)\u001b[39m.\u001b[39mfrom_bytes(doc_like)\n\u001b[0;32m-> 1124\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE1041\u001b[39m.\u001b[39mformat(\u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39mtype\u001b[39m(doc_like)))\n",
      "\u001b[0;31mValueError\u001b[0m: [E1041] Expected a string, Doc, or bytes as input, but got: <class 'float'>"
     ]
    }
   ],
   "source": [
    "df['preprocessed_text'] = df['Text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
